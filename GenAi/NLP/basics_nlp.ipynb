{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zH0zH9tWSL4Z"
   },
   "source": [
    "word_tokenize():this will break the sentence with the whitespace which seperates punctuation too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 382,
     "status": "ok",
     "timestamp": 1731553110977,
     "user": {
      "displayName": "gayathri Telaprolu",
      "userId": "11640461603257869832"
     },
     "user_tz": -330
    },
    "id": "LjY0A4mt0Y9d",
    "outputId": "ed4ec02e-9bb7-4d5e-9c60-2b76b4c46772"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'helps', 'in', 'breaking', 'down', 'sentences', 'into', 'individual', 'words', 'or', 'tokens', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Download the 'punkt_tab' resource\n",
    "nltk.download('punkt_tab')\n",
    "# Sample sentence\n",
    "document = \"Tokenization helps in breaking down sentences into individual words or tokens.\"\n",
    "\n",
    "# Perform word tokenization\n",
    "tokens = word_tokenize(document)\n",
    "print(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgP4WaEhTMDi"
   },
   "source": [
    "WhitespaceTokenizer:this will break the sentence by whitespace which does not seperates any punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1731553110978,
     "user": {
      "displayName": "gayathri Telaprolu",
      "userId": "11640461603257869832"
     },
     "user_tz": -330
    },
    "id": "x7q2-E-UQBLr",
    "outputId": "80e4e823-9156-40be-b4b1-f2532f84cf7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'helps', 'in', 'breaking', 'down', 'sentences', 'into', 'individual', 'words', 'or', 'tokens.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "# Sample text\n",
    "text = \"Tokenization helps in breaking down sentences into individual words or tokens.\"\n",
    "\n",
    "# Create a WhitespaceTokenizer instance\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Print the tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nk_93Tl5mdsk"
   },
   "source": [
    "Removing punctuation using string method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1731553110978,
     "user": {
      "displayName": "gayathri Telaprolu",
      "userId": "11640461603257869832"
     },
     "user_tz": -330
    },
    "id": "Gt2p4D8pQEYP",
    "outputId": "8b91f65f-3bb2-4696-aa6f-c371548c2bca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'helps', 'in', 'breaking', 'down', 'sentences', 'into', 'individual', 'words', 'or', 'tokens']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Sample text\n",
    "text = \"Tokenization helps in breaking down, sentences into individual words or tokens.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Remove punctuation tokens\n",
    "tokens_without_punctuation = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "# Print the tokens without punctuation\n",
    "print(tokens_without_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0mELH7cRsQE"
   },
   "source": [
    "TweetTokenizer(): This tokenizer is specifically designed to handle the nuances of social media text, like hashtags, mentions, and emoticons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1731553110978,
     "user": {
      "displayName": "gayathri Telaprolu",
      "userId": "11640461603257869832"
     },
     "user_tz": -330
    },
    "id": "Z1LsBWFcRqiV",
    "outputId": "65cc6f5b-509a-41a7-eba1-fd6877615375"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@nltk', ':', 'Check', 'out', 'this', 'awesome', '#NLP', 'library', '!', ':)']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "text = \"RT @nltk: Check out this awesome #NLP library! :)\"\n",
    "\n",
    "tknzr = TweetTokenizer()\n",
    "tokens = tknzr.tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaOICpdMSBSD"
   },
   "source": [
    "MWETokenizer([('New', 'York', 'City')]): This tokenizer allows you to treat multi-word expressions (like \"New York City\") as single tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1731553110979,
     "user": {
      "displayName": "gayathri Telaprolu",
      "userId": "11640461603257869832"
     },
     "user_tz": -330
    },
    "id": "_ey-AkuSR35P",
    "outputId": "b466ae2c-c36b-4725-930e-e19a77114923"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['New_York_City', 'is', 'a', 'great', 'place', 'to', 'visit', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "text = \"New York City is a great place to visit.\"\n",
    "\n",
    "tokenizer = MWETokenizer([('New', 'York', 'City')])  # Define multi-word expression\n",
    "tokens = tokenizer.tokenize(nltk.word_tokenize(text))\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 297,
     "status": "ok",
     "timestamp": 1731554520515,
     "user": {
      "displayName": "gayathri Telaprolu",
      "userId": "11640461603257869832"
     },
     "user_tz": -330
    },
    "id": "yCB005iVpOwQ",
    "outputId": "5fe3f40f-912e-4b05-ed46-3ebe9427a030"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'important', 'benefit', 'resulting', 'from', 'the', 'popularity', 'of', 'Natural', 'Language', 'Processing', 'NLP', 'is', 'increased', 'connections', 'and', 'collaborative', 'discussion', 'across', 'different', 'audiences—from', 'industries', 'such', 'as', 'healthcare', 'education', 'and', 'retail', 'or', 'job', 'roles', 'like', 'data', 'scientist', 'Chief', 'Technology', 'Officer', 'or', 'integrated', 'marketing', 'manager', 'to', 'name', 'a', 'few', 'Whether', 'you', 'understand', 'the', 'nuances', 'of', 'NLP', 'or', 'you', '’', 're', 'just', 'learning', 'about', 'the', 'technology', 'this', 'resource', 'list', 'includes', 'a', 'bit', 'of', 'something', 'for', 'everyone', 'There', 'are', 'fewer', '“', 'traditional', '”', 'blogs', 'for', 'NLP', 'but', 'what', '’', 's', 'apparent', 'is', 'how', 'many', 'resources', 'reflect', 'ongoing', 'dialogue', 'and', 'represent', 'a', 'cross-section', 'of', 'the', 'technology', 'If', 'you', '’', 're', 'so', 'inclined', 'after', 'taking', 'a', 'look', 'at', 'some', 'of', 'these', 'offer', 'your', 'own', 'perspective', 'on', 'NLP', 'and', 'join', 'the', 'discussion']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "corpus=\"One important benefit resulting from the popularity of Natural Language Processing (NLP) is increased connections and collaborative discussion across different audiences—from industries such as healthcare, education, and retail or job roles like data scientist, Chief Technology Officer, or integrated marketing manager, to name a few. Whether you understand the nuances of NLP or you’re just learning about the technology, this resource list includes a bit of something for everyone. There are fewer “traditional” blogs for NLP, but what’s apparent is how many resources reflect ongoing dialogue and represent a cross-section of the technology. If you’re so inclined after taking a look at some of these, offer your own perspective on NLP and join the discussion.\"\n",
    "\n",
    "\n",
    "sentences = word_tokenize(corpus)\n",
    "tokens_without_punctuation = [token for token in sentences if token not in string.punctuation]\n",
    "print(tokens_without_punctuation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRXZeVhQtg-p"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 141,
     "status": "ok",
     "timestamp": 1731554830275,
     "user": {
      "displayName": "gayathri Telaprolu",
      "userId": "11640461603257869832"
     },
     "user_tz": -330
    },
    "id": "6Mk3Qs8VsZEw",
    "outputId": "dedb7387-0751-473a-c753-690bad72f1bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "dog\n",
      "is\n",
      "play\n",
      "with\n",
      "the\n",
      "ball\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "document=\"The dog is playing with the ball\"\n",
    "ps=PorterStemmer()\n",
    "words=word_tokenize(document)\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYYQTLTZtjhk"
   },
   "source": [
    "Here when I use Stemming for the unique word 'playing'  the ending of the word is chopped to get it root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 256,
     "status": "ok",
     "timestamp": 1731554832679,
     "user": {
      "displayName": "gayathri Telaprolu",
      "userId": "11640461603257869832"
     },
     "user_tz": -330
    },
    "id": "WYi6dMSPswlG",
    "outputId": "f011bc48-6db4-48af-ec6e-88f6729145be"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "dog\n",
      "is\n",
      "playing\n",
      "with\n",
      "the\n",
      "ball\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "for w in words:\n",
    "    print(lemmatizer.lemmatize(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJkETzs6t1R8"
   },
   "source": [
    "When I done lemmatization there is no change in the unique word 'playing' beacuse its dictionary meaning will changed if there is any change in 'playing'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 173,
     "status": "ok",
     "timestamp": 1731555162458,
     "user": {
      "displayName": "gayathri Telaprolu",
      "userId": "11640461603257869832"
     },
     "user_tz": -330
    },
    "id": "ZD6Qfq5-uPwg",
    "outputId": "47ba7a8b-9e6d-49e1-dd02-8f07e41fce5a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'dog', 'playing', 'ball']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words=set(stopwords.words('english'))\n",
    "#print(stop_words)\n",
    "with_out_stop_words=[w for w in words if w not in stop_words]\n",
    "print(with_out_stop_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0XLeeMKv0FO"
   },
   "source": [
    "By using stopwords, the unique words which are with the less meaning are deleted and the words from which actual context of the document is taken for the future processing of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 209,
     "status": "ok",
     "timestamp": 1731555351072,
     "user": {
      "displayName": "gayathri Telaprolu",
      "userId": "11640461603257869832"
     },
     "user_tz": -330
    },
    "id": "v2SAqdXRu4Hf",
    "outputId": "80b3893d-b383-47aa-8c65-b1753b6fde22"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Error with downloaded zip file\n"
     ]
    },
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pos_tag\n\u001b[0;32m      3\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maveraged_perceptron_tagger\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwith_out_stop_words\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\tag\\__init__.py:168\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos_tag\u001b[39m(tokens, tagset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\tag\\__init__.py:110\u001b[0m, in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    108\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m PerceptronTagger(lang\u001b[38;5;241m=\u001b[39mlang)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\tag\\perceptron.py:183\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[1;34m(self, load, lang)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\tag\\perceptron.py:273\u001b[0m, in \u001b[0;36mPerceptronTagger.load_from_json\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;66;03m# Automatically find path to the tagger if location is not specified.\u001b[39;00m\n\u001b[1;32m--> 273\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaggers/averaged_perceptron_tagger_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(loc \u001b[38;5;241m+\u001b[39m TAGGER_JSONS[lang][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(fin)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\data.py:551\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    549\u001b[0m modified_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(pieces[:i] \u001b[38;5;241m+\u001b[39m [pieces[i] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m pieces[i:])\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodified_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\data.py:538\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(p):\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 538\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mZipFilePathPointer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzipentry\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m    540\u001b[0m         \u001b[38;5;66;03m# resource not in zipfile\u001b[39;00m\n\u001b[0;32m    541\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\data.py:391\u001b[0m, in \u001b[0;36mZipFilePathPointer.__init__\u001b[1;34m(self, zipfile, entry)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;124;03mCreate a new path pointer pointing at the specified entry\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;124;03min the given zipfile.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;124;03mdoes not contain the specified entry.\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(zipfile, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 391\u001b[0m     zipfile \u001b[38;5;241m=\u001b[39m \u001b[43mOpenOnDemandZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzipfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;66;03m# Check that the entry exists:\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m entry:\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;66;03m# Normalize the entry string, it should be relative:\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\data.py:1020\u001b[0m, in \u001b[0;36mOpenOnDemandZipFile.__init__\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filename, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1019\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReopenableZipFile filename must be a string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1020\u001b[0m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename \u001b[38;5;241m==\u001b[39m filename\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\zipfile.py:1299\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m-> 1299\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RealGetContents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1300\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m   1301\u001b[0m         \u001b[38;5;66;03m# set the modified flag so central directory gets written\u001b[39;00m\n\u001b[0;32m   1302\u001b[0m         \u001b[38;5;66;03m# even if no files are added to the archive\u001b[39;00m\n\u001b[0;32m   1303\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_didModify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\zipfile.py:1366\u001b[0m, in \u001b[0;36mZipFile._RealGetContents\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m endrec:\n\u001b[1;32m-> 1366\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1368\u001b[0m     \u001b[38;5;28mprint\u001b[39m(endrec)\n",
      "\u001b[1;31mBadZipFile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "pos_tag(with_out_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fNl8fLEwOA5"
   },
   "source": [
    "By using POS tagging, the grammatical structure of the document without stop words are given.\n",
    "\n",
    "NN: Noun (e.g., “dog,” “car”)\n",
    "VB: Verb (e.g., “run,” “jump”)\n",
    "JJ: Adjective (e.g., “quick,” “lazy”)\n",
    "RB: Adverb (e.g., “quickly,” “slowly”)\n",
    "DT: Determiner (e.g., “the,” “a”)\n",
    "IN: Preposition (e.g., “in,” “over”)\n",
    "PRP: Pronoun (e.g., “he,” “she”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 157,
     "status": "ok",
     "timestamp": 1731555946489,
     "user": {
      "displayName": "gayathri Telaprolu",
      "userId": "11640461603257869832"
     },
     "user_tz": -330
    },
    "id": "P2LDiuDuxLXL",
    "outputId": "79694b3f-c575-4969-95e4-1cd755b5eda9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'import', 'benefit', 'result', 'popular', 'natur', 'languag', 'process', 'nlp', 'increas', 'connect', 'collabor', 'discuss', 'across', 'differ', 'audiences—from', 'industri', 'healthcar', 'educ', 'retail', 'job', 'role', 'like', 'data', 'scientist', 'chief', 'technolog', 'offic', 'integr', 'market', 'manag', 'name', 'whether', 'understand', 'nuanc', 'nlp', '’', 'learn', 'technolog', 'resourc', 'list', 'includ', 'bit', 'someth', 'everyon', 'there', 'fewer', '“', 'tradit', '”', 'blog', 'nlp', '’', 'appar', 'mani', 'resourc', 'reflect', 'ongo', 'dialogu', 'repres', 'cross-sect', 'technolog', 'if', '’', 'inclin', 'take', 'look', 'offer', 'perspect', 'nlp', 'join', 'discuss']\n",
      "72\n",
      "123\n"
     ]
    }
   ],
   "source": [
    "corpus_stemming=[]\n",
    "for w in tokens_without_punctuation:\n",
    "    if w not in stop_words:\n",
    "        corpus_stemming.append(ps.stem(w))\n",
    "print(corpus_stemming)\n",
    "print(len(corpus_stemming))\n",
    "print(len(tokens_without_punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 209,
     "status": "ok",
     "timestamp": 1731555995192,
     "user": {
      "displayName": "gayathri Telaprolu",
      "userId": "11640461603257869832"
     },
     "user_tz": -330
    },
    "id": "jeibbMXLxzuJ",
    "outputId": "bbf2d77f-d7a1-4f0b-c4f0-254ef372bfca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'important', 'benefit', 'resulting', 'popularity', 'Natural', 'Language', 'Processing', 'NLP', 'increased', 'connection', 'collaborative', 'discussion', 'across', 'different', 'audiences—from', 'industry', 'healthcare', 'education', 'retail', 'job', 'role', 'like', 'data', 'scientist', 'Chief', 'Technology', 'Officer', 'integrated', 'marketing', 'manager', 'name', 'Whether', 'understand', 'nuance', 'NLP', '’', 'learning', 'technology', 'resource', 'list', 'includes', 'bit', 'something', 'everyone', 'There', 'fewer', '“', 'traditional', '”', 'blog', 'NLP', '’', 'apparent', 'many', 'resource', 'reflect', 'ongoing', 'dialogue', 'represent', 'cross-section', 'technology', 'If', '’', 'inclined', 'taking', 'look', 'offer', 'perspective', 'NLP', 'join', 'discussion']\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "corpus_lemmatization=[]\n",
    "for w in tokens_without_punctuation:\n",
    "    if w not in stop_words:\n",
    "        corpus_lemmatization.append(lemmatizer.lemmatize(w))\n",
    "print(corpus_lemmatization)\n",
    "print(len(corpus_lemmatization))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 168,
     "status": "ok",
     "timestamp": 1731556196028,
     "user": {
      "displayName": "gayathri Telaprolu",
      "userId": "11640461603257869832"
     },
     "user_tz": -330
    },
    "id": "tBmWF6gGx8mB",
    "outputId": "d2c988e1-61fa-47a1-8c15-e9edc719fe5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'list', 'look', 'traditional', 'data', 'If', 'There', 'scientist', 'reflect', 'industries', 'education', 'important', 'Technology', 'Officer', '’', 'blogs', 'represent', 'healthcare', 'cross-section', '“', 'integrated', 'nuances', 'ongoing', 'technology', 'understand', 'connections', 'job', 'name', 'bit', 'retail', 'roles', 'manager', 'learning', 'popularity', 'benefit', 'fewer', '”', 'apparent', 'something', 'inclined', 'Natural', 'many', 'Language', 'discussion', 'like', 'taking', 'audiences—from', 'offer', 'dialogue', 'resources', 'marketing', 'Processing', 'resulting', 'includes', 'NLP', 'across', 'join', 'collaborative', 'resource', 'everyone', 'different', 'perspective', 'Whether', 'Chief', 'increased', 'One'}\n",
      "65\n",
      "['One', 'important', 'benefit', 'resulting', 'popularity', 'Natural', 'Language', 'Processing', 'NLP', 'increased', 'connections', 'collaborative', 'discussion', 'across', 'different', 'audiences—from', 'industries', 'healthcare', 'education', 'retail', 'job', 'roles', 'like', 'data', 'scientist', 'Chief', 'Technology', 'Officer', 'integrated', 'marketing', 'manager', 'name', 'Whether', 'understand', 'nuances', 'NLP', '’', 'learning', 'technology', 'resource', 'list', 'includes', 'bit', 'something', 'everyone', 'There', 'fewer', '“', 'traditional', '”', 'blogs', 'NLP', '’', 'apparent', 'many', 'resources', 'reflect', 'ongoing', 'dialogue', 'represent', 'cross-section', 'technology', 'If', '’', 'inclined', 'taking', 'look', 'offer', 'perspective', 'NLP', 'join', 'discussion']\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "corpus_stopwords=[]\n",
    "for w in tokens_without_punctuation:\n",
    "    if w not in stop_words:\n",
    "        corpus_stopwords.append(w)\n",
    "set_corpus_stopword=set(corpus_stopwords)\n",
    "print(set_corpus_stopword)\n",
    "print(len(set_corpus_stopword))\n",
    "print(corpus_stopwords)\n",
    "print(len(corpus_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 190,
     "status": "ok",
     "timestamp": 1731556238991,
     "user": {
      "displayName": "gayathri Telaprolu",
      "userId": "11640461603257869832"
     },
     "user_tz": -330
    },
    "id": "B5RBN9YbyDMs",
    "outputId": "489e56ed-0c4b-4f49-dbcb-d980718d3cd4"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus_stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m corpus_pos_tag\u001b[38;5;241m=\u001b[39mpos_tag(\u001b[43mcorpus_stopwords\u001b[49m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(corpus_pos_tag)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'corpus_stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "corpus_pos_tag=pos_tag(corpus_stopwords)\n",
    "print(corpus_pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 206,
     "status": "ok",
     "timestamp": 1731556503511,
     "user": {
      "displayName": "gayathri Telaprolu",
      "userId": "11640461603257869832"
     },
     "user_tz": -330
    },
    "id": "05zIxpJwzm-b",
    "outputId": "411d7fc3-264d-4f92-f98f-4436c397c380"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus_stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m----> 2\u001b[0m word_freq \u001b[38;5;241m=\u001b[39m Counter(\u001b[43mcorpus_stopwords\u001b[49m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Display the word frequency\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord Frequency:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'corpus_stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_freq = Counter(corpus_stopwords)\n",
    "\n",
    "# Display the word frequency\n",
    "print(\"Word Frequency:\")\n",
    "for word, freq in word_freq.items():\n",
    "    print(f\"{word}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "executionInfo": {
     "elapsed": 965,
     "status": "ok",
     "timestamp": 1731556624905,
     "user": {
      "displayName": "gayathri Telaprolu",
      "userId": "11640461603257869832"
     },
     "user_tz": -330
    },
    "id": "A-AfCtmZ0PRx",
    "outputId": "a49a847f-489f-4394-ef06-4f2ce27f47a8"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Generate word cloud\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "# Generate word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Hide the axes\n",
    "plt.title('Word Frequency Word Cloud')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP/A5rUSgVQYay9J0SEQT7+",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
