{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using REgular Expression(Regex):Regular expressions are like patterns to search for specific parts of text\n",
    "1. Extracting all the mails from the document.\n",
    "2. Replacing gidit with [NUM].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emails: ['support@example.com', 'sales@domain.org']\n",
      "Masked Text: Contact us at support@example.com or sales@domain.org. Call [NUM][NUM][NUM]-[NUM][NUM][NUM]-[NUM][NUM][NUM][NUM].\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Contact us at support@example.com or sales@domain.org. Call 123-456-7890.\"\n",
    "emails = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
    "print(\"Emails:\", emails)\n",
    "\n",
    "masked_text = re.sub(r'\\d', '[NUM]', text)\n",
    "print(\"Masked Text:\", masked_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using different types of tokenizers. Tokenizing means breaking text into smaller parts like words or sentences.\n",
    "1.word_tokenize: break the text into unique words\n",
    "2.sent_tokenize: break the text into sentences by using the whitespace.\n",
    "3.blankline_tokenize: used when we are using large paragraphs when each paragraph is seperated by the blanklines and then we will get each paragraph and then we can perform sent_tokenize and then on sent_tokenize we can perform word_tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['Hello', '!', 'How', 'are', 'you', '?', 'I', 'am', 'learning', 'NLP', '.', 'It', \"'s\", 'exciting', '.']\n",
      "Sentence Tokens: ['Hello!', 'How are you?', 'I am learning NLP.', \"It's exciting.\"]\n",
      "Blankline Tokens: ['Hello! How are you?', \"I am learning NLP.\\nIt's exciting.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize, blankline_tokenize\n",
    "\n",
    "text = \"Hello! How are you?\\n\\nI am learning NLP.\\nIt's exciting.\"\n",
    "print(\"Word Tokens:\", word_tokenize(text))\n",
    "print(\"Sentence Tokens:\", sent_tokenize(text))\n",
    "print(\"Blankline Tokens:\", blankline_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Bigrams, Trigrams, and Ngrams: These are groups of 2, 3, or more words in a row, called bigrams, trigrams, or ngrams.\n",
    "Generating word sequences of different lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams: [('I', 'am'), ('am', 'learning'), ('learning', 'NLP'), ('NLP', 'and'), ('and', 'it'), ('it', 'is'), ('is', 'fun'), ('fun', '.')]\n",
      "Trigrams: [('I', 'am', 'learning'), ('am', 'learning', 'NLP'), ('learning', 'NLP', 'and'), ('NLP', 'and', 'it'), ('and', 'it', 'is'), ('it', 'is', 'fun'), ('is', 'fun', '.')]\n",
      "4-grams: [('I', 'am', 'learning', 'NLP'), ('am', 'learning', 'NLP', 'and'), ('learning', 'NLP', 'and', 'it'), ('NLP', 'and', 'it', 'is'), ('and', 'it', 'is', 'fun'), ('it', 'is', 'fun', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"I am learning NLP and it is fun.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "trigrams = list(ngrams(tokens, 3))\n",
    "ngrams_4 = list(ngrams(tokens, 4))\n",
    "\n",
    "print(\"Bigrams:\", bigrams)\n",
    "print(\"Trigrams:\", trigrams)\n",
    "print(\"4-grams:\", ngrams_4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StopWords: Remove common words to focus on meaningful ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Sentence: ['example', 'removing', 'stopwords', 'sentence', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sentence = \"This is an example of removing stopwords from a sentence.\"\n",
    "filtered_words = [word for word in word_tokenize(sentence) if word.lower() not in stop_words]\n",
    "print(\"Filtered Sentence:\", filtered_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " UTF Encoding, URLs, and Hashtags\n",
    " Handle different encodings, URLs, and social media text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLs: ['https://example.com']\n",
      "Hashtags: ['#NLP']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Check out our website https://example.com and #NLP is amazing!\"\n",
    "urls = re.findall(r'http[s]?://\\S+', text)\n",
    "hashtags = re.findall(r'#\\w+', text)\n",
    "\n",
    "print(\"URLs:\", urls)\n",
    "print(\"Hashtags:\", hashtags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS Tagging: Giving tags for words like verb,noun,adnerb etc. TO know there importance in the sentnce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('The', 'DT'), ('dog', 'NN'), ('runs', 'VBZ'), ('fast', 'RB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "sentence = \"The dog runs fast.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "print(\"POS Tags:\", pos_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DT: Determiner\n",
    "NN: Noun\n",
    "VBZ: Verb\n",
    "RB: Adverb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Named Entity Recognition (NER)\n",
    "NER identifies names, places, or other special entities in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities: (S\n",
      "  (PERSON Barack/NNP)\n",
      "  (PERSON Obama/NNP)\n",
      "  was/VBD\n",
      "  born/VBN\n",
      "  in/IN\n",
      "  (GPE Hawaii/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "sentence = \"Barack Obama was born in Hawaii.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "ner = nltk.ne_chunk(pos_tags)\n",
    "\n",
    "print(\"Named Entities:\", ner)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PERSON: Person name\n",
    "GPE: Geopolitical entity (like a location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UTF (Unicode Transformation Format) encoding is a standard way to represent text in computers. It ensures that characters from different languages and symbols can be stored and displayed consistently.\n",
    "\n",
    "Universal Compatibility: Handles characters from multiple languages (e.g., English, Hindi, Chinese).\n",
    "Consistent Storage: Ensures text looks the same across systems.\n",
    "Avoid Data Loss: Prevents errors like \"�\" (replacement character) when dealing with special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTF-8 Encoded: b'Hello, \\xe4\\xbd\\xa0\\xe5\\xa5\\xbd, \\xe0\\xa4\\xa8\\xe0\\xa4\\xae\\xe0\\xa4\\xb8\\xe0\\xa5\\x8d\\xe0\\xa4\\xa4\\xe0\\xa5\\x87!'\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, 你好, नमस्ते!\"  # English, Chinese, Hindi\n",
    "utf8_encoded = text.encode('utf-8')\n",
    "print(\"UTF-8 Encoded:\", utf8_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Text: Hello, 你好, नमस्ते!\n"
     ]
    }
   ],
   "source": [
    "decoded_text = utf8_encoded.decode('utf-8')\n",
    "print(\"Decoded Text:\", decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored: b'Hello, !'\n",
      "Replaced: b'Hello, ?!'\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, 🌟!\"  # Includes a special star emoji\n",
    "\n",
    "# Ignore unencodable characters\n",
    "ignored = text.encode('ascii', errors='ignore')\n",
    "print(\"Ignored:\", ignored)\n",
    "\n",
    "# Replace unencodable characters\n",
    "replaced = text.encode('ascii', errors='replace')\n",
    "print(\"Replaced:\", replaced)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping text data from a website and kept that text in to the text file , to perform chunking and chinking on that text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped text has been saved to 'scraped_text.txt'\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# URL of the website\n",
    "url = \"https://www.digitalocean.com/resources/articles/ai-customer-service\"\n",
    "\n",
    "# Send a GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check the status code to ensure the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Use the CSS selector to find the element\n",
    "    element = soup.select_one(\n",
    "        \"#__next > div.Layout__StyledLayout-sc-4d75cf92-0.ckzcZt > div:nth-child(3) > div > div > div > div.SidebarLayoutStyles__StyledSidebarLayoutContent-sc-f3937a35-6.fYXiyA\"\n",
    "    )\n",
    "\n",
    "    if element:\n",
    "        # Extract each paragraph (<p>) separately\n",
    "        paragraphs = element.find_all(\"p\")  # Find all <p> tags within the element\n",
    "        \n",
    "        # Open a file to save the paragraphs\n",
    "        with open(\"scraped_text.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "            for paragraph in paragraphs:\n",
    "                # Write each paragraph into the file with a newline between paragraphs\n",
    "                file.write(paragraph.get_text(strip=True) + \"\\n\\n\")\n",
    "        \n",
    "        print(\"Scraped text has been saved to 'scraped_text.txt'\")\n",
    "    else:\n",
    "        print(\"Element not found!\")\n",
    "else:\n",
    "    print(f\"Failed to fetch the webpage. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking: Extracting meaning full noun phrase (or) verb pharse from the document by giving grammer syntax and applying this grammer syntax on the document by using\" regularexpression\". And if there is a big paragraph if we give limit of each chunk as 100 words and the chunking process will chunk the text in to different chunk with each chunk of 100 words.\n",
    "\n",
    "Chinking: It will remove unwanted words from the chunked data here unwanted data means like prepositions and d=some other meaningless words presemt in that text .Same as the chunking by giving the particular grammar synatx for chinking then that will be applied on the chunked data by \"regular expression\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "no.of chunked words: 10\n",
      "no.of chinked words: 11\n",
      "Chunks:\n",
      "- The quick brown\n",
      "- fox\n",
      "- jumps\n",
      "- the lazy dog\n",
      "- was\n",
      "- sitting\n",
      "- the tree\n",
      "- was a sunny day\n",
      "- were\n",
      "- playing\n",
      "\n",
      "Chinked Words:\n",
      "- over\n",
      "- ,\n",
      "- who\n",
      "- by\n",
      "- .\n",
      "- It\n",
      "- ,\n",
      "- and\n",
      "- the\n",
      "- children\n",
      "- .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, RegexpParser\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\" The quick brown fox jumps over the lazy dog, who was sitting by the tree.\n",
    "    It was a sunny day, and the children were playing.\"\"\"\n",
    "\n",
    "# Tokenize and POS tagging\n",
    "tokens = word_tokenize(text)\n",
    "tagged = pos_tag(tokens)\n",
    "\n",
    "# Define Chunking rules\n",
    "chunking_grammar = \"\"\"\n",
    "    NP: {<DT>?<JJ>*<NN>}   # Noun Phrase (optional determiner, adjectives, noun)\n",
    "    VP: {<VB.*><NP|PP>*}    # Verb Phrase (verb + noun phrase or prepositional phrase)\n",
    "\"\"\"\n",
    "# Define Chinking rules\n",
    "chinking_grammar = \"\"\"\n",
    "    NP: {<DT>?<JJ>*<NN>} }<IN|CC>+{  # Chinking (removes prepositions and conjunctions from NP)\n",
    "\"\"\"\n",
    "\n",
    "# Create the parser\n",
    "chunk_parser = RegexpParser(chunking_grammar)\n",
    "\n",
    "# Apply the chunking and chinking\n",
    "tree = chunk_parser.parse(tagged)\n",
    "\n",
    "# Extract chunked and chinked words\n",
    "chunks = []\n",
    "chinked_words = []\n",
    "for subtree in tree:\n",
    "    if isinstance(subtree, nltk.Tree):\n",
    "        chunks.append(\" \".join(word for word, tag in subtree.leaves()))\n",
    "    else:\n",
    "        chinked_words.append(subtree[0])\n",
    "\n",
    "# Output\n",
    "print(len(tagged))\n",
    "print(\"no.of chunked words:\",len(chunks))\n",
    "print(\"no.of chinked words:\",len(chinked_words))\n",
    "print(\"Chunks:\")\n",
    "for chunk in chunks:\n",
    "    print(f\"- {chunk}\")\n",
    "\n",
    "print(\"\\nChinked Words:\")\n",
    "for word in chinked_words:\n",
    "    print(f\"- {word}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here chunking and chinking is done on the scraped text data, giving output as a file with the chunked data and no of words removed by chinking and the chunked data after removing the chinking words from that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text has been chunked and chinked into 24 chunks of 100 words each.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Define a chunking grammar for noun phrases (NP) and verb phrases (VP)\n",
    "chunking_grammar = r\"\"\"\n",
    "    NP: {<DT>?<JJ>*<NN>}  # Noun Phrase: Optional determiner, adjectives, and noun\n",
    "    VP: {<VB.*><NP|PP>*}  # Verb Phrase: Verb followed by noun or prepositional phrases\n",
    "\"\"\"\n",
    "\n",
    "# Define a chinking grammar to remove certain patterns from the chunks\n",
    "chinking_grammar = r\"\"\"\n",
    "    NP: {<DT>?<JJ>*<NN>}   # Define the initial noun phrase\n",
    "        }<IN|CC>+{         # Remove any prepositions or conjunctions\n",
    "\"\"\"\n",
    "\n",
    "# Create chunk and chink parsers\n",
    "chunk_parser = nltk.RegexpParser(chunking_grammar)\n",
    "chink_parser = nltk.RegexpParser(chinking_grammar)\n",
    "\n",
    "\n",
    "def chunk_text_and_chinking(file_path, chunk_size, output_file):\n",
    "    # Read the text file\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "\n",
    "    # Create chunks of the specified size (e.g., 100 words per chunk)\n",
    "    chunks = [words[i:i + chunk_size] for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "    # Write the chunked and chinked text to an output file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as out_file:\n",
    "        for index, chunk in enumerate(chunks, 1):\n",
    "            # Tokenize the chunk into words and perform POS tagging\n",
    "            tokenized_chunk = nltk.word_tokenize(\" \".join(chunk))\n",
    "            tagged_chunk = nltk.pos_tag(tokenized_chunk)\n",
    "\n",
    "            # Apply chunking (using the defined grammar)\n",
    "            chunked_tree = chunk_parser.parse(tagged_chunk)\n",
    "\n",
    "            # Apply chinking (to refine the chunked tree)\n",
    "            chinked_tree = chink_parser.parse(chunked_tree.leaves())\n",
    "\n",
    "            # Calculate chinked words\n",
    "            chinked_words = [\n",
    "                word for word, pos in chunked_tree.leaves()\n",
    "                if not any(word in leaf[0] for leaf in chinked_tree.leaves())\n",
    "            ]\n",
    "\n",
    "            # Write the results to the output file\n",
    "            out_file.write(f\"Chunk {index}:\\n\")\n",
    "            out_file.write(f\"Original Chunk: {' '.join(chunk)}\\n\")\n",
    "            out_file.write(\"Chunked Text:\\n\")\n",
    "\n",
    "            for subtree in chunked_tree.subtrees():\n",
    "                if subtree.label() in [\"NP\", \"VP\"]:\n",
    "                    out_file.write(\" \".join(word for word, pos in subtree.leaves()) + \"\\n\")\n",
    "\n",
    "            out_file.write(\"\\nChinked Words:\\n\")\n",
    "            out_file.write(\", \".join(chinked_words) + \"\\n\")\n",
    "            out_file.write(f\"Number of Chinked Words: {len(chinked_words)}\\n\")\n",
    "\n",
    "            # New chunk after chinking\n",
    "            refined_chunk = \" \".join(word for word, pos in chinked_tree.leaves())\n",
    "            out_file.write(f\"Refined Chunk After Chinking: {refined_chunk}\\n\\n\")\n",
    "\n",
    "    print(f\"Text has been chunked and chinked into {len(chunks)} chunks of {chunk_size} words each.\")\n",
    "\n",
    "\n",
    "# Parameters\n",
    "input_file = \"scraped_text.txt\"\n",
    "output_file = \"chunked_and_chinking_text.txt\"\n",
    "chunk_size = 100  # Number of words per chunk\n",
    "\n",
    "# Execute the function\n",
    "chunk_text_and_chinking(input_file, chunk_size, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words (BoW)\n",
    "Definition: Bag of Words is one of the simplest text representation methods. It represents text data as a collection (or \"bag\") of words, where:\n",
    "\n",
    "Order of words does not matter (which is why it's called a \"bag\").\n",
    "Duplicate words are counted (so the frequency of each word in the text is considered).\n",
    "CountBag of Words (BoW)\n",
    "Definition: Bag of Words is one of the simplest text representation methods. It represents text data as a collection (or \"bag\") of words, where:\n",
    "\n",
    "Order of words does not matter (which is why it's called a \"bag\").\n",
    "Duplicate words are counted (so the frequency of each word in the text is considered).\n",
    "CountVectorizer:\n",
    "Used to represent the bag of words in the matrix form by using scikit library.\n",
    "Term frequency: no.of times term(t) will appear in that document(d)\n",
    "Inverse Document Frequency: for total no.of documents mean(corpus) how many times does the term is repeating(t)\n",
    "1. if IDF is more mean the word is repeating more, so that we can give less importance to that term.\n",
    "TF-IDF == TF*IDF here we can get the term that is frequent in that document but rare in the corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fun' 'is' 'learning' 'love' 'machine']\n",
      "[[0 0 1 1 1]\n",
      " [1 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\"I love machine learning\", \"Machine learning is fun\"]\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Transform the corpus into a document-term matrix (DTM)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Display the feature names (vocabulary)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the document-term matrix (counts of each word)\n",
    "print(X.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
